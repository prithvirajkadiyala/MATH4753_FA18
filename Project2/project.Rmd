---
title: "MATH 4753 Project 2"
author: "Prithviraj Kadiyala"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    csl: biomed-central.csl
    df_print: paged
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    toc: yes
    toc_depth: 4
bibliography: project.bib
abstract: This project is all about applications of SLR to real data using R. I will be utilizing a dataset that contains information about employees in a company with years of experience and their current salary. My project data is taken from an online source of sample dataset of Employees vs Salary. I will be using textbook and the information from all the labs covered until now to process and analyze the data to see how the salary changes according to the years of experience of the employee. To check if the years of experience even matters for thte hike in the salary. Goal is to apply SLR(Simple Linear Regression) to examine the relationship between the two entities.
---

<center>

![Prithviraj Kadiyala](images/prithviraj.jpg){ width=30% }

</center>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(dplyr)
```

# My Video

<video controls>
<source src="test1.mp4" type = "video/mp4">
</video>


# Introduction

Salary - A fixed regular payment, typically paid on a monthly or biweekly basis but often expressed as an annual sum, made by an employer to an employee, especially a professional or white-collar worker. 
<center>
![Salary](images/Employee_1.png){ width=30% }
</center>

The following was taken from [Forbes Articles](https://www.forbes.com/sites/cameronkeng/2014/06/22/employees-that-stay-in-companies-longer-than-2-years-get-paid-50-less/#3ae687b8e07f)

There has been a lot of buzz going around in the software industry about unequal pays to the employees who have been working for a single company longer. and Who recently graduated and gets very good salary. There also have been a lot of articles written about unequal pays to the loyal employees and employees changing their jobs every 3-4 years getting almost 50% hike in salaries.

Those very new to the tech industry, with less than a year of experience, can expect to earn \$50,321 (a year-over-year increase of 9.8 percent). After a year or two, that average salary jumps to \$62,517 (a whooping 24.3 percent increase, year-over-year).

Spend three to five years, and the average leaps yet again, to \$68,040 (a 6.3 percent increase). Between six and ten years in the industry, salaries hit $83,143 (a rise of 6.8 percent).

Breaking the ten-year mark translates into big bucks. Those with 11 to 15 years of experience could expect to pull down \$96,792 (a 3.8 percent increase over last year), while those with more than 15 years average \$115,399 (a 6 percent increase).

Below is the graph that shows us the salary hike when employees jump companies:
![Salary](images/Employee_2.jpg){ width=100% }


The data was collected here:

```{r map, echo=FALSE, fig.align='center'}
leaflet() %>%
  setView(lng=-97.445717, lat=35.205894, zoom= 16) %>% 
  addTiles() %>%
  addMarkers(lng=-97.447227, lat=35.209281, popup="We collected data using internet, sitting in the class.") 
```


## What are the variables? 
```{r datacars}
dataset = read.csv("Emp_Salary.csv",header=TRUE,sep=",")
head(dataset)
```

```{r carnames}
names(dataset)
```


### Plot data
```{r}
library(s20x)
pairs20x(dataset)
```

```{r }
library(ggplot2)
g = ggplot(dataset, aes(x = YrsExper, y = Salary, color = EducLev)) + geom_point()
g = g + xlab("Years of Experience") 
g = g + geom_smooth(method = "loess")
g
```


## How were the data collected? 

## What is the story behind the data?

## Why was it gathered? 

## What is your interest in the data?
With the prospects of working in the software industry in the future. It would really cool to analyze the working of the IT industry beforehand and be prepared with what to do and when to do given the circumstances can put me into really good perspective of getting into the market and negotiating for a higher base salary package.

## What problem do you wish to solve?
I would like to look into the dataset and try to relate if the salary is dependent on years of experience directly. If not, then do we see a lot of variation in the graph as the years of xperience increases. Some people clain that only technical knowledge will take us to a position that pays us well. It's true to some extent but we will study about how having experience will have an impact on one's salary.
So the problem we define today is going to be "Is years of experience alone enough to increase your salary or does having years and years of experience doesnt have an effect on your salary at all".

# Theory needed to carry out SLR
I believe that the value of Salary (Y) tends to increase as Years of Experience (X), that is, when the employee has increasing years of experience his salary also tends to increase. I want to make a model relating the two variables to one another by drawing a line through all the points. I will define salary as my dependent variable, and years of expeirence as my independent variable.

I could use a deterministic model if all the data points were perfectly aligned and I didn't have to worry about errors in my prediction; however, I know from the preliminary graphs that the data points are not perfectly aligned. A probabilistic model will be more accurate, in this instance, as it will take into account the randomness of the distribution of data points around the line. A simple linear regression model (hereafter referred to as SLR) is one type of probabilistic model, and will be used in my data analysis. SLR assumes that the mean value of the y data for any value of the x data will make a straight line when graphed, and that any points which deviate from the line (above or below) are equal to ??. This statement is written as:

$$
y= \beta_0 +\beta_1x_i+\epsilon
$$

When $\beta_0$ and $\beta_1$ are unknown parameters, $\beta_0+\beta_1x_i$ is the mean value of y for a given x and $\epsilon$ is the random error. Working with the assumption that some points are going to deviate from the line, I know that some will be above(positive deviation) and some below(negative deviation), with an $E(\epsilon)=0$. That would make the mean value of y:
$$
\begin{align}
E(y)&=E(\beta_0+\beta_1x_i+\epsilon_i)\\
&=\beta_0+\beta_1x_i+E(\epsilon_i)\\
&=\beta_0+\beta_1x_i
\end{align}
$$

Thus, the mean value of y for any given value of x will be represented by $E(Y|x)$ and will graph as a straight line , with an intercept of $\beta_0$ and a slope of $\beta_1$

The regression has five key assumptions:


>
  1. Linear Relationship
  2. Multivariate Normality
  3. No or little multicollinearity
  4. No Auto co-relation
  5. Homoscedasticity
  

# Validity with mathematical expressions
In order to estimate $\beta_0$ and $\beta_1$ we are going to use the method of least squares.As discussed in class this helps us determine the line that best fits our data points with the minimum sum of squares of the deviations. This is called the SSE or Sum of Squares for Errors. In a straight line model, we have already discussed that $y= \beta_0 +\beta_1x_i+\epsilon$. The estimator will be $\hat y= \hat\beta_0 +\hat \beta_1x_i$ . The residual(the deviation of the ith valueof y from its predicted value) is calculated bvy $(y_i-\hat y_i) = y_i-(\hat\beta_0+\hat\beta_1x_i)$. Thus $SSE=\sum^n_{i=1}[y_i-(\hat\beta_0+\hat\beta_1x_i)]$

If the model works well with our data then we should expect that the residuals are approximately normal in distribution with mean = 0 and a constant variance.


The following function was taken from [https://rpubs.com/therimalaya/43190](https://rpubs.com/therimalaya/43190)

## Checks on validity

##Creating a linear model
```{r}
dataset.lm=lm(Salary~YrsExper,data=dataset)
summary(dataset.lm)
```


We can get the values for the Intercept($\beta_0$) and the estimate($\beta_1$) from the summary above.
$$
\begin{align}
\beta_0 &= 30329.73\\
\beta_1 &= 991.64
\end{align}
$$

## calculating the Confience Interval for Parameter Estimates
```{r}
ciReg(dataset.lm, conf.level=0.95, print.out=TRUE)
```


###Least squares estimates
$$
\begin{align}
\hat\beta_0 + \hat\beta_1x_i &= 30329.73 + 991.64* x_i
\end{align}
$$

The least-squares estimate of the slope, $\hat\beta_1=991.64$, indicates that the estimated amount of increase of salary increases by 992\$ for every year increase, with this interpretation being valid over the range of years of experience values. We can see that the increase in salary is dependent on Years of Experience. But lets also conduct a test with a quadratic model to fit the curve better and see if that will help us get more clearer on the result we just got.



#Verifying Assumptions
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Residual Line Segments of Salary vs YrsExper", data=dataset)
abline(dataset.lm)
```

##Plot of Residuals
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Residual Line Segments of Salary vs YrsExper", data=dataset)
ht.lm=with(dataset, lm(Salary~YrsExper))
abline(ht.lm)
yhat=with(dataset,predict(ht.lm,data.frame(YrsExper)))
with(dataset,{segments(YrsExper,Salary,YrsExper,yhat)})
abline(ht.lm)
```

##Plot of Mean
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
             ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
             main="Mean of Salary vs YrsExper", data=dataset)
abline(dataset.lm)
with(dataset, abline(h=mean(Salary)))
with(dataset, segments(YrsExper,mean(Salary),YrsExper,yhat,col="Red"))
```

##Plot of means with total deviations from the Line Segments
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Total Deviation Line Segments of Salary vs YrsExper", data=dataset)
with(dataset,abline(h=mean(Salary)))
with(dataset, segments(YrsExper,Salary,YrsExper,mean(Salary),col="Green"))
```

##Using MSS, RSS and TSS
```{r}
RSS=with(dataset,sum((Salary-yhat)^2))
RSS
```

```{r}
MSS=with(dataset,sum((yhat-mean(Salary))^2))
MSS
```

```{r}
TSS=with(dataset,sum((Salary-mean(Salary))^2))
TSS
```

$R^2$ is equal to $\frac{MSS}{TSS}$, which means that the value calculated is the value for the trend line. The closer $R^2$ is to 1, the better the fit of the trend line.

```{r}
MSS/TSS
```

This value indicates that the trend line is not a good fit for the data i have.

##Trendscatter
```{r}
trendscatter(Salary~YrsExper,f=0.5,data=dataset)
```

Here, we use trendscatter to look and get the feel of how the data is scattered. Then according to the scatter we will do specific analysis in order to get the best results for our research interest.

We can see that the data is concentrated towards the line and there are fewer and fewer datapoints as we move away from the trend line.
By this we can say that the error is distributed pretty normal. But again this is only by the visual inspection and we will ahev to perform more analysis that will give us more accurate resultss. First thig we would do is try to get a linear model and see how it looks and go forward from there.

##Find the residuals and Fitted Values
```{r}
Yrs.res=residuals(dataset.lm)
Yrs.fit=fitted(dataset.lm)
```


##Residuals vs Yrs of Experience values
```{r}
plot(dataset$YrsExper,Yrs.res, xlab="YrsExper",ylab="Residuals",ylim=c(-1.5*max(Yrs.res),1.5*max(Yrs.res)),xlim=c(0,1.6*max(Yrs.fit)), main="Residuals vs Yrsof Experience")
```


It looks as though the residuals are somewhat about the zero on the y-axis, but the values are mostly aligned towards the bottom of zero,
this indicates that there is no SIGNIFICANT deviation from the line of best-fit.

## Trendscatter on Residual Vs Fitted
```{r}
trendscatter(Yrs.fit,Yrs.res, xlab="Fitted", ylab="Residuals") 
```


## Shapiro-wilk
```{r}
normcheck(dataset.lm,shapiro.wilk = TRUE)
```

The p-value for the shapiro-wilk test is 0. The null hypothesis in this case would be that the errors are distributed normally.

$$\epsilon_i \sim N(0,\sigma^2)$$

The results of the Shapiro-wilk test indicate that we have enough evidence against to reject the null hypothesis(as the p-value is 0 compared to the standard of comparison 0.05) leading us to the conclusion that the data is not normally distributed.


# Testing another model for comparison
<center>
$y=\hat\beta_0+\hat\beta_1x_i+\hat\beta_2x^2_i$
</center>

```{r}
quad.lm=lm(Salary~YrsExper + I(YrsExper^2),data=dataset)

plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
   ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),main="Scatter Plot and Quadratic of Salary vs YrsExper",data=dataset)
myplot = function(x){quad.lm$coef[1] + quad.lm$coef[2]*x + quad.lm$coef[3]*x^2}
curve(myplot, lwd = 2, add = TRUE)
```

Fitting a quadratic to the data does produce a visibly dissimilar result; it does not look completely linear, as we have seen prior to this during the linear model plot, but further analysis will clarify the results and allow us to make a final decision.

```{r}
quad.fit = c(Yrs.fit)
```

##A Plot of the Residuals versus Fitted Values
```{r}
plot(quad.lm, which = 1)
```

```{r}
normcheck(quad.lm, shapiro.wilk = TRUE)
```

The p-value is 0. Again, the results of the Shapiro-Wilk test indicate that we DO have enough evidence to reject the null hypothesis, leading us to again assume that the data is NOT distributed normally.

##Summarize the model
```{r}
summary(quad.lm)
```

Here we have the following values
$$
\begin{align}
\beta_0 &= 36378.79\\
\beta_1 &= -199.45\\
\beta_2 &= 38.49
\end{align}
$$

And further we have the R-squared values in the summary whch we caan use further to compare with the linear model earlier to see which of the model fits the data better.

##Calculting the Confidence Interval
```{r}
ciReg(quad.lm, conf.level=0.95, print.out=TRUE)
```

So the equation comes out to be 
$$
\begin{align}
\beta_0 + \beta_1*x_i+\beta_2*x^2_i&= 36378.79 -199.45*x_i + 38.49*x^2_i
\end{align}
$$

# Model selection
##Making Predictions using the Two Models
###For Model1
```{r}
amount = predict(dataset.lm, data.frame(YrsExper=c(10,25,40)))
amount
```

###For Model2
```{r}
amount2 = predict(quad.lm, data.frame(YrsExper=c(10,25,40)))
amount2
```

The predictions made using the first model (linear) are greater in the beginning and smaller in the end compared to the predictions made by the second model (quadratic), but they are extremely close to one another. Further comparisons will be necessary to determine which model will be the best fit for the dataset.

## Use adjusted $R^2$ 
```{r}
summary(dataset.lm)
summary(quad.lm)
```

The multiple $R^2$ for the linear model is 0.379; the adjusted $R^2$ is 0.376. The multiple $R^2$ for the quadratic model is 0.4468; the adjusted $R^2$ is 0.4414.

According to these results, both the models have a low multiple $R^2$ value. But the quadratic linear model has a significantly higher value which tells us that the Quadratic model fits the data better than the Linear model. "Linear regression calculates an equation that minimizes the distance between the fitted line and all of the data points. Technically, least squares regression minimizes the sum of the squared residuals." ![Website link](http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)

### Check on outliers using cooks plots

Remember to interpret this plot and all other plots



# Conclusion
## Answer your research question
## Suggest ways to improve model or experiment


# References
  Salary Pic - https://insights.dice.com/2016/02/11/how-much-will-experience-increase-my-salary/ 
  Graph - https://www.forbes.com/sites/cameronkeng/2014/06/22/employees-that-stay-in-companies-longer-than-2-years-get-paid-50-less/#1fb8fb8be07f
  Assumptions of Linear Regression - https://www.statisticssolutions.com/assumptions-of-linear-regression/ 
