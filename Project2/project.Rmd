---
title: "MATH 4753 Project 2"
author: "Prithviraj Kadiyala"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    csl: biomed-central.csl
    df_print: paged
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    toc: yes
    toc_depth: 4
bibliography: project.bib
abstract: This project is all about applications of SLR to real data using R. I will be utilizing a dataset that contains information about employees in a company with years of experience and their current salary. My project data is taken from an online source of sample dataset of Employees vs Salary. I will be using textbook and the information from all the labs covered until now to process and analyze the data to see how the salary changes according to the years of experience of the employee. To check if the years of experience even matters for thte hike in the salary. Goal is to apply SLR(Simple Linear Regression) to examine the relationship between the two entities.
---

<center>

![Prithviraj Kadiyala](images/prithviraj.jpg){ width=30% }

</center>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(dplyr)
```

# My Video

<video controls>
<source src="test1.mp4" type = "video/mp4">
</video>


# Introduction

Salary - A fixed regular payment, typically paid on a monthly or biweekly basis but often expressed as an annual sum, made by an employer to an employee, especially a professional or white-collar worker. 
<center>
![Salary](images/Employee_1.png){ width=30% }
</center>

The following was taken from [Forbes Articles](https://www.forbes.com/sites/cameronkeng/2014/06/22/employees-that-stay-in-companies-longer-than-2-years-get-paid-50-less/#3ae687b8e07f)

There has been a lot of buzz going around in the software industry about unequal pays to the employees who have been working for a single company longer. and Who recently graduated and gets very good salary. There also have been a lot of articles written about unequal pays to the loyal employees and employees changing their jobs every 3-4 years getting almost 50% hike in salaries.

Those very new to the tech industry, with less than a year of experience, can expect to earn \$50,321 (a year-over-year increase of 9.8 percent). After a year or two, that average salary jumps to \$62,517 (a whooping 24.3 percent increase, year-over-year).

Spend three to five years, and the average leaps yet again, to \$68,040 (a 6.3 percent increase). Between six and ten years in the industry, salaries hit $83,143 (a rise of 6.8 percent).

Breaking the ten-year mark translates into big bucks. Those with 11 to 15 years of experience could expect to pull down \$96,792 (a 3.8 percent increase over last year), while those with more than 15 years average \$115,399 (a 6 percent increase).

Below is the graph that shows us the salary hike when employees jump companies:
![Salary](images/Employee_2.jpg){ width=100% }


The data was collected here:

```{r map, echo=FALSE, fig.align='center'}
leaflet() %>%
  setView(lng=-97.445717, lat=35.205894, zoom= 16) %>% 
  addTiles() %>%
  addMarkers(lng=-97.447227, lat=35.209281, popup="We collected data using internet, sitting in the class.") 
```


## What are the variables? 
```{r datacars}
dataset = read.csv("Emp_Salary.csv",header=TRUE,sep=",")
head(dataset)
```

The variables we use here are 

* Employees - Discrete, Qualitative
* YrsExper - Discrete, Quantitative
* Salary - Discrete, Quantitative
* EducLev - Discrete, Qualitative

```{r names}
names(dataset)
```


### Plot data
```{r}
library(s20x)
pairs20x(dataset)
```

```{r }
library(ggplot2)
g = ggplot(dataset, aes(x = YrsExper, y = Salary, color = EducLev)) + geom_point()
g = g + xlab("Years of Experience") 
g = g + geom_smooth(method = "loess")
g
```


## How was the data collected? 
The data is taken from a sample data from the website [Kaggle database website](https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset)


## What is the story behind the data?
Being a computer Engineer graduate student, I was looking into how I could get the most money after graduating. I also wanted to see how much does years of experience does it take to make \$100K if i work in a single company. Also if i can't get to \$100K after working for many years in a single comapany. What would it take to make that kind of money.

## Why was it gathered? 
This data was gathered to give an idea for the job-seekers of how much salary should be expected after having some Years of Experience. Even though salary might depend on various other factors ($salary = base + var * experience * performance$) as mentioned in this website [here](https://ask.metafilter.com/209665/Salary-Vs-experience-analysis).


## What is your interest in the data?
With the prospects of working in the software industry in the future. It would really cool to analyze the working of the IT industry beforehand and be prepared with what to do and when to do given the circumstances can put me into really good perspective of getting into the market and negotiating for a higher base salary package.

## What problem do you wish to solve?
I would like to look into the dataset and try to relate if the salary is dependent on years of experience directly. If not, then do we see a lot of variation in the graph as the years of xperience increases. Some people clain that only technical knowledge will take us to a position that pays us well. It's true to some extent but we will study about how having experience will have an impact on one's salary.
So the problem we define today is going to be "Does the Salary have linear relation with respect to Years of Experience?".

# Theory needed to carry out SLR
I believe that the value of Salary (Y) tends to increase as Years of Experience (X), that is, when the employee has increasing years of experience his salary also tends to increase. I want to make a model relating the two variables to one another by drawing a line through all the points. I will define salary as my dependent variable, and years of expeirence as my independent variable.

I could use a deterministic model if all the data points were perfectly aligned and I didn't have to worry about errors in my prediction; however, I know from the preliminary graphs that the data points are not perfectly aligned. A probabilistic model will be more accurate, in this instance, as it will take into account the randomness of the distribution of data points around the line. A simple linear regression model (hereafter referred to as SLR) is one type of probabilistic model, and will be used in my data analysis. SLR assumes that the mean value of the y data for any value of the x data will make a straight line when graphed, and that any points which deviate from the line (above or below) are equal to ??. This statement is written as:

$$
y= \beta_0 +\beta_1x_i+\epsilon
$$

When $\beta_0$ and $\beta_1$ are unknown parameters, $\beta_0+\beta_1x_i$ is the mean value of y for a given x and $\epsilon$ is the random error. Working with the assumption that some points are going to deviate from the line, I know that some will be above(positive deviation) and some below(negative deviation), with an $E(\epsilon)=0$. That would make the mean value of y:
$$
\begin{align}
E(y)&=E(\beta_0+\beta_1x_i+\epsilon_i)\\
&=\beta_0+\beta_1x_i+E(\epsilon_i)\\
&=\beta_0+\beta_1x_i
\end{align}
$$

Thus, the mean value of y for any given value of x will be represented by $E(Y|x)$ and will graph as a straight line , with an intercept of $\beta_0$ and a slope of $\beta_1$

The regression has five key assumptions:


>
  1. Linear Relationship
  2. Multivariate Normality
  3. No or little multicollinearity
  4. No Auto co-relation
  5. Homoscedasticity
  

# Validity with mathematical expressions
In order to estimate $\beta_0$ and $\beta_1$ we are going to use the method of least squares.As discussed in class this helps us determine the line that best fits our data points with the minimum sum of squares of the deviations. This is called the SSE or Sum of Squares for Errors. In a straight line model, we have already discussed that $y= \beta_0 +\beta_1x_i+\epsilon$. The estimator will be $\hat y= \hat\beta_0 +\hat \beta_1x_i$ . The residual(the deviation of the ith valueof y from its predicted value) is calculated bvy $(y_i-\hat y_i) = y_i-(\hat\beta_0+\hat\beta_1x_i)$. Thus $SSE=\sum^n_{i=1}[y_i-(\hat\beta_0+\hat\beta_1x_i)]$

If the model works well with our data then we should expect that the residuals are approximately normal in distribution with mean = 0 and a constant variance.

## Checks on validity

##Creating a linear model
```{r}
dataset.lm=lm(Salary~YrsExper,data=dataset)
summary(dataset.lm)
```


We can get the values for the Intercept($\beta_0$) and the estimate($\beta_1$) from the summary above.
$$
\begin{align}
\beta_0 &= 30329.73\\
\beta_1 &= 991.64
\end{align}
$$

## calculating the Confience Interval for Parameter Estimates
```{r}
ciReg(dataset.lm, conf.level=0.95, print.out=TRUE)
```


###Least squares estimates
$$
\begin{align}
\hat\beta_0 + \hat\beta_1x_i &= 30329.73 + 991.64* x_i
\end{align}
$$

The least-squares estimate of the slope, $\hat\beta_1=991.64$, indicates that the estimated amount of increase of salary increases by 992\$ for every year increase, with this interpretation being valid over the range of years of experience values. We can see that the increase in salary is dependent on Years of Experience. But lets also conduct a test with a quadratic model to fit the curve better and see if that will help us get more clearer on the result we just got.



#Verifying Assumptions
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Residual Line Segments of Salary vs YrsExper", data=dataset)
abline(dataset.lm)
```

##Plot of Residuals
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Residual Line Segments of Salary vs YrsExper", data=dataset)
ht.lm=with(dataset, lm(Salary~YrsExper))
abline(ht.lm)
yhat=with(dataset,predict(ht.lm,data.frame(YrsExper)))
with(dataset,{segments(YrsExper,Salary,YrsExper,yhat)})
abline(ht.lm)
```

##Plot of Mean
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
             ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
             main="Mean of Salary vs YrsExper", data=dataset)
abline(dataset.lm)
with(dataset, abline(h=mean(Salary)))
with(dataset, segments(YrsExper,mean(Salary),YrsExper,yhat,col="Red"))
```

##Plot of means with total deviations from the Line Segments
```{r}
plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
              ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),
              main="Total Deviation Line Segments of Salary vs YrsExper", data=dataset)
with(dataset,abline(h=mean(Salary)))
with(dataset, segments(YrsExper,Salary,YrsExper,mean(Salary),col="Green"))
```

##Using MSS, RSS and TSS
```{r}
RSS=with(dataset,sum((Salary-yhat)^2))
RSS
```

```{r}
MSS=with(dataset,sum((yhat-mean(Salary))^2))
MSS
```

```{r}
TSS=with(dataset,sum((Salary-mean(Salary))^2))
TSS
```

$R^2$ is equal to $\frac{MSS}{TSS}$, which means that the value calculated is the value for the trend line. The closer $R^2$ is to 1, the better the fit of the trend line.

```{r}
MSS/TSS
```

This value indicates that the trend line is not a good fit for the data i have.

##Trendscatter
```{r}
trendscatter(Salary~YrsExper,f=0.5,data=dataset)
```

Here, we use trendscatter to look and get the feel of how the data is scattered. Then according to the scatter we will do specific analysis in order to get the best results for our research interest.

We can see that the data is concentrated towards the line and there are fewer and fewer datapoints as we move away from the trend line.
By this we can say that the error is distributed pretty normal. But again this is only by the visual inspection and we will ahev to perform more analysis that will give us more accurate resultss. First thig we would do is try to get a linear model and see how it looks and go forward from there.

##Find the residuals and Fitted Values
```{r}
Yrs.res=residuals(dataset.lm)
Yrs.fit=fitted(dataset.lm)
```


##Residuals vs Yrs of Experience values
```{r}
plot(dataset$YrsExper,Yrs.res, xlab="YrsExper",ylab="Residuals",ylim=c(-1.5*max(Yrs.res),1.5*max(Yrs.res)),xlim=c(0,max(dataset$YrsExper)), main="Residuals vs Yrsof Experience")
```


It looks as though the residuals are somewhat about the zero on the y-axis, but the values are mostly aligned towards the bottom of zero,
this indicates that there is no SIGNIFICANT deviation from the line of best-fit.

## Trendscatter on Residual Vs Fitted
```{r}
trendscatter(Yrs.fit,Yrs.res, xlab="Fitted", ylab="Residuals") 
```


## Shapiro-wilk
```{r}
normcheck(dataset.lm,shapiro.wilk = TRUE)
```

The p-value for the shapiro-wilk test is 0. The null hypothesis in this case would be that the errors are distributed normally.

```
Since the Normality test is very sensitive to large amount of data. We assume that the errors are normlly distriuted.

```

# Testing another model for comparison
<center>
$y=\hat\beta_0+\hat\beta_1x_i+\hat\beta_2x^2_i$
</center>

```{r}
quad.lm=lm(Salary~YrsExper + I(YrsExper^2),data=dataset)

plot(Salary~YrsExper,bg="Blue",pch=21,cex=1.2,
   ylim=c(0,1.1*max(Salary)),xlim=c(0,1.1*max(YrsExper)),main="Scatter Plot and Quadratic of Salary vs YrsExper",data=dataset)
myplot = function(x){quad.lm$coef[1] + quad.lm$coef[2]*x + quad.lm$coef[3]*x^2}
curve(myplot, lwd = 2, add = TRUE)
```

Fitting a quadratic to the data does produce a visibly dissimilar result; it does not look completely linear, as we have seen prior to this during the linear model plot, but further analysis will clarify the results and allow us to make a final decision.

```{r}
quad.fit = c(Yrs.fit)
```

##A Plot of the Residuals versus Fitted Values
```{r}
plot(quad.lm, which = 1)
```

```{r}
normcheck(quad.lm, shapiro.wilk = TRUE)
```

```
Again we see that we get P-value to be zero because the normality test is very sesnsitive to the large data.
We again assume that errors are normally distributed.
```

The p-value is 0. Again, the results of the Shapiro-Wilk test indicate that we DO have enough evidence to reject the null hypothesis, leading us to again assume that the data is NOT distributed normally.

##Summarize the model
```{r}
summary(quad.lm)
```

Here we have the following values
$$
\begin{align}
\beta_0 &= 36378.79\\
\beta_1 &= -199.45\\
\beta_2 &= 38.49
\end{align}
$$

And further we have the R-squared values in the summary whch we caan use further to compare with the linear model earlier to see which of the model fits the data better.

##Calculting the Confidence Interval
```{r}
ciReg(quad.lm, conf.level=0.95, print.out=TRUE)
```

So the equation comes out to be 
$$
\begin{align}
\beta_0 + \beta_1*x_i+\beta_2*x^2_i&= 36378.79 -199.45*x_i + 38.49*x^2_i
\end{align}
$$

# Model selection
##Making Predictions using the Two Models
###For Model1
```{r}
amount = predict(dataset.lm, data.frame(YrsExper=c(10,25,40)))
amount
```

###For Model2
```{r}
amount2 = predict(quad.lm, data.frame(YrsExper=c(10,25,40)))
amount2
```

The predictions made using the first model (linear) are greater in the beginning and smaller in the end compared to the predictions made by the second model (quadratic), but they are extremely close to one another. Further comparisons will be necessary to determine which model will be the best fit for the dataset.

## Comparing adjusted $R^2$ 
```{r}
summary(dataset.lm)
summary(quad.lm)
```

The multiple $R^2$ for the linear model is 0.379; the adjusted $R^2$ is 0.376. The multiple $R^2$ for the quadratic model is 0.4468; the adjusted $R^2$ is 0.4414.

According to these results, both the models have a low multiple $R^2$ value. But the quadratic linear model has a significantly higher value which tells us that the Quadratic model fits the data better than the Linear model. "Linear regression calculates an equation that minimizes the distance between the fitted line and all of the data points. Technically, least squares regression minimizes the sum of the squared residuals." ![Website link](http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit). The adjusted $R^2$ is greater for the Quadratic model than the Linear model which tells us that the Quadratic model can fit more data into the model or can predict with more accuracy than the Linear model.

##Using Anova for Confirmation
To avoid any premature decisions it is always suggested to make the final calculation to make the final decision. So we try to do an anova test to confirm if what we did was right or wrong.
```{r}
anova(dataset.lm,quad.lm)
```

Seeing the results, we can see that the p-value is too small causing us to reject the null hypothesis as there is very large evidence against the null-hypothesis that the Linear model is better than the quadratic model.
So after this confirmation we can say that Quadratic model obviously is better than the Linear model.

But again before concluding, we are going to use cook's plot to avoid having any bias towards any kind of result.


## Check on outliers using cooks plots
```{r}
cooks20x(quad.lm)
```


```{r}
dataset2.lm=lm(Salary~YrsExper+ I(YrsExper^2), data=dataset[-208,])
summary(dataset2.lm)
summary(quad.lm)
```

After looking at the summaries of the standard Quadratic model and the model after taking out the maximum cook's value from the dataset variable. We can see that the Residual Standard Error has decreased from 8413 to 7851. And also the adjusted $R^2$ increase from 0.4414 to 0.514 giving us a better chance at predicting new data or just making sure that more of the data is included in the curve. Also we can see that the F-statistic value increases from 82.77 to 109.9 giving us a better result in general.



# Conclusion
Concluding the models and the results we got from the models.
We did get two different graphs and predictions from two models. After analyzing the predictions and the summaries of the values after taking the cook's plot values and deleting the most significant value in the Quadratic model. We achieved a better result in the form of better Adjusted multiple $R^2$ value and reduced error value. We also got better predictions with the Quadratic model than the Linear model so we can say that the Quadratic model is better suited for the data we had today for our analysis. Cook's plot gave us a clear view of some of the data that was causing the graph to favour some data points and when we took out that outlier the multiple $R^2$ value seems to increase a bit making room for more values to be taken closer to the predicted line.


## Answer your research question
Research questin here was "Does the Salary have linear relation with respect to Years of Experience?"
According to all of the reasearch and analysis done until now we can clearly see that the salary does increase with the increase of years of experience of the employee but we also see that the increase is not linear. As the employee gains more and more experience his/her salary seems to increase exponentially. Of course there can be other factors that can impact the result.


## Suggest ways to improve model or experiment
This experiment of predicting salary can be improved by taking many otherd factors into consideration like Degree of the employee, Proficiency of the employee, Technical knowledge, etc. The dataset we have today consists of only 200 odd number of records, so if we want to have a much more correct estimation we need to have more data regarding this matter and account for all the possible values in different categories.


# References
  Salary differences - https://insights.dice.com/2016/02/11/how-much-will-experience-increase-my-salary/ 
  
  Graph of Increase in Salary - https://www.forbes.com/sites/cameronkeng/2014/06/22/employees-that-stay-in-companies-longer-than-2-years-get-paid-50-less/#1fb8fb8be07f
  
  Assumptions of Linear Regression - https://www.statisticssolutions.com/assumptions-of-linear-regression/ 
  
  Forbes Articles - https://www.forbes.com/sites/cameronkeng/2014/06/22/employees-that-stay-in-companies-longer-than-2-years-get-paid-50-less/#3ae687b8e07f
  
  Dataset website - https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset
  
  Simple Linear Regression - https://onlinecourses.science.psu.edu/stat501/node/251/ 
  
  Tools to calcuate salaries - https://www.recruiter.com/i/10-great-salary-calculators-to-save-you-time/
  
  Textbook - Mendenhall, W.M. and Sincich, T.L. (2016). Statistics for Engineering and the Sciences, Sixth Edition. https://books.google.com/books?id=OHNGrgEACAAJ. isbn=9781498728850,lccn=2015041987. 
  
  Canvas - https://canvas.ou.edu/courses/84540 